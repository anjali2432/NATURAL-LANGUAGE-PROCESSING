{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 2 : Find the word frequency."
      ],
      "metadata": {
        "id": "90c8mlAC2q4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count(elements):\n",
        "  if elements[-1] ==\".\":\n",
        "    elements = elements[0:len(elements) - 1]\n",
        "  if elements in dictionary :\n",
        "    dictionary[elements] += 1\n",
        "\n",
        "  else:\n",
        "    dictionary.update({elements: 1})\n",
        "\n",
        "sentence = \"Apple Mango Orange Mango Apple Papaya banana Mango orange\"\n",
        "dictionary = {}\n",
        "\n",
        "lst = sentence.split()\n",
        "\n",
        "for elements in lst:\n",
        "      count(elements)\n",
        "\n",
        "for allkeys in dictionary :\n",
        "  print(\"Frequency of\", allkeys,end = \" \")\n",
        "  print(\":\",end = \" \")\n",
        "  print(dictionary[allkeys],end = \" \")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "est6QJpxzYYG",
        "outputId": "7d19bcff-839a-4c27-c51e-d431c8f542a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency of Apple : 2 \n",
            "Frequency of Mango : 3 \n",
            "Frequency of Orange : 1 \n",
            "Frequency of Papaya : 1 \n",
            "Frequency of banana : 1 \n",
            "Frequency of orange : 1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count(elements):\n",
        "  if elements[-1] ==\".\":\n",
        "    elements = elements[0:len(elements) - 1]\n",
        "  if elements in dictionary :\n",
        "    dictionary[elements] += 1\n",
        "\n",
        "  else:\n",
        "    dictionary.update({elements: 1})\n",
        "\n",
        "sentence = input(\"Enter your Sentence : \")\n",
        "dictionary = {}\n",
        "\n",
        "lst = sentence.split()\n",
        "\n",
        "for elements in lst:\n",
        "      count(elements)\n",
        "\n",
        "for allkeys in dictionary :\n",
        "  print(\"Frequency of\", allkeys,end = \" \")\n",
        "  print(\":\",end = \" \")\n",
        "  print(dictionary[allkeys],end = \" \")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcmqAjz920nq",
        "outputId": "5adcacc0-9eb3-4fe8-ac17-b57c1b1c468e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Sentence : A a B H j D A A\n",
            "Frequency of A : 3 \n",
            "Frequency of a : 1 \n",
            "Frequency of B : 1 \n",
            "Frequency of H : 1 \n",
            "Frequency of j : 1 \n",
            "Frequency of D : 1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "B6fYMlLb6Uw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "# single word lemmatization\n",
        "list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling',\n",
        "\t\t'driving', 'died', 'tried', 'feet']\n",
        "for words in list1:\n",
        "\tprint(words + \" ---> \" + wnl.lemmatize(words))"
      ],
      "metadata": {
        "id": "6ZcUsnoM1pUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a62a8117-a5ef-44a2-85bb-7fd336e40681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kites ---> kite\n",
            "babies ---> baby\n",
            "dogs ---> dog\n",
            "flying ---> flying\n",
            "smiling ---> smiling\n",
            "driving ---> driving\n",
            "died ---> died\n",
            "tried ---> tried\n",
            "feet ---> foot\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming"
      ],
      "metadata": {
        "id": "v8HzDSqo77ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import these modules\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# choose some words to be stemmed\n",
        "words = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\"]\n",
        "\n",
        "for w in words:\n",
        "\tprint(w, \" : \", ps.stem(w))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_Bq0NfS6PwJ",
        "outputId": "9ea5d3e4-aab9-4d0d-9067-4861a181a717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "program  :  program\n",
            "programs  :  program\n",
            "programmer  :  programm\n",
            "programming  :  program\n",
            "programmers  :  programm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing modules\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "sentence = \"Programmers program with programming languages\"\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "for w in words:\n",
        "\tprint(w, \" : \", ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcGf_-xa7aSv",
        "outputId": "4218d236-3c5c-488f-b69c-febcd0d5454d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Programmers  :  programm\n",
            "program  :  program\n",
            "with  :  with\n",
            "programming  :  program\n",
            "languages  :  languag\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "n = 2\n",
        "\n",
        "sentence = \"The purpose of life is to happy\"\n",
        "bigrams = ngrams(sentence.split(),n)\n",
        "print(\"Bigram Model is :\\n\")\n",
        "for items in bigrams:\n",
        "  print(items)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgXbLfsz-CqB",
        "outputId": "e17e5c54-243c-4f46-a84b-9a5359798420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram Model is :\n",
            "\n",
            "('The', 'purpose')\n",
            "('purpose', 'of')\n",
            "('of', 'life')\n",
            "('life', 'is')\n",
            "('is', 'to')\n",
            "('to', 'happy')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "n = 3\n",
        "\n",
        "sentence = \"The purpose of life is to happy\"\n",
        "trigrams = ngrams(sentence.split(),n)\n",
        "print(\"trigram Model is :\\n\")\n",
        "for items in trigrams:\n",
        "  print(items)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09O7AHld_EVA",
        "outputId": "ab7be321-c8d0-4c8e-842f-b4ea755d3f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trigram Model is :\n",
            "\n",
            "('The', 'purpose', 'of')\n",
            "('purpose', 'of', 'life')\n",
            "('of', 'life', 'is')\n",
            "('life', 'is', 'to')\n",
            "('is', 'to', 'happy')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def generate_regex(text):\n",
        "  regex=re.escape(text)\n",
        "  regex=regex.replace(r'\\\\', r'\\\\s+')\n",
        "  return regex\n",
        "\n",
        "\n",
        "text = \"This is a sample text\"\n",
        "regex = generate_regex(text)\n",
        "print(f\"Text is : {text}\")\n",
        "print(f\"Regex : {regex}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecO7gE3q_cwX",
        "outputId": "517061ca-56ee-4a16-cfc0-3cb7b522d6ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text is : This is a sample text\n",
            "Regex : This\\ is\\ a\\ sample\\ text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def identify_parts_of_speech(text):\n",
        "  words = word_tokenize(text)\n",
        "  pos_tags = nltk.pos_tag(words)\n",
        "  return pos_tags\n",
        "\n",
        "Text = \"This is a simple sentence\"\n",
        "parts_of_speech = identify_parts_of_speech(text)\n",
        "print(parts_of_speech)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7HkFWg4CZkv",
        "outputId": "81867115-e2ee-4f86-8f7a-65ef2c7d3ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('text', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import hmm\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('treebank')\n",
        "tagged_sentences = treebank.tagged_sents()\n",
        "train_data = tagged_sentences[:3000]\n",
        "test_data = tagged_sentences[3000:]\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "hmm_model = trainer.train(train_data)\n",
        "accuracy = hmm_model.evaluate(test_data)\n",
        "print(f\"Accuracy of model is :{accuracy :.2f}%\")\n",
        "sample_sentence = \"This is a sample sentence\"\n",
        "sample_tokens = nltk.word_tokenize(sample_sentence)\n",
        "predicted_tags = hmm_model.tag(sample_tokens)\n",
        "print(\"predicted pos tags: \")\n",
        "print(predicted_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riKdH1_aHcMj",
        "outputId": "c866eb7c-5578-44c4-e752-ec4347108279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "<ipython-input-14-b90d16ec6052>:13: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  accuracy = hmm_model.evaluate(test_data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of model is :0.37%\n",
            "predicted pos tags: \n",
            "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'NNP'), ('sentence', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import hmm\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('treebank')\n",
        "tagged_sentences = treebank.tagged_sents()\n",
        "train_data = tagged_sentences[:30000]\n",
        "test_data = tagged_sentences[3000:]\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "hmm_model = trainer.train(train_data)\n",
        "accuracy = hmm_model.evaluate(test_data)\n",
        "print(f\"Accuracy of model is :{accuracy:.2f}%\")\n",
        "sample_sentence = \"This is a sample sentence\"\n",
        "sample_tokens = nltk.word_tokenize(sample_sentence)\n",
        "predicted_tags = hmm_model.tag(sample_tokens)\n",
        "print(\"predicted pos tags: \")\n",
        "print(predicted_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID0jux1ZM87s",
        "outputId": "91374067-e5c2-4135-e814-6894860cbdb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "<ipython-input-19-7a2870410ba0>:13: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  accuracy = hmm_model.evaluate(test_data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of model is :0.98%\n",
            "predicted pos tags: \n",
            "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'NNP'), ('sentence', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import RegexpParser\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import treebank\n",
        "nltk.download('punkt')\n",
        "nltk.download('treebank')\n",
        "\n",
        "tagged_sentences = treebank.tagged_sents()\n",
        "train_data = tagged_sentences[:3000]\n",
        "pos_tagger = nltk.DefaultTagger('NN')\n",
        "pos_tagger = nltk.UnigramTagger(train_data,backoff=pos_tagger)\n",
        "sample_sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens = word_tokenize(sample_sentence)\n",
        "pos_tags = pos_tagger.tag(tokens)\n",
        "Chunk_grammar = r\"\"\"\n",
        "        NP:{<DT>?<JJ>+<NN>}\n",
        "        VP:{<VB*><NP><PP>*}\n",
        "        \"\"\"\n",
        "\n",
        "Chunk_parser = RegexpParser(Chunk_grammar)\n",
        "tree = Chunk_parser.parse(pos_tags)\n",
        "tree.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fr_T4QKOAzw",
        "outputId": "8feba14b-d497-430e-a7eb-7173f8b2787d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                  S                                       \n",
            "   _______________________________|_________________________               \n",
            "  |       |        |      |       |      |     |            NP            \n",
            "  |       |        |      |       |      |     |     _______|________      \n",
            "fox/NN jumps/NN over/IN the/DT lazy/NN dog/NN ./. The/DT quick/JJ brown/NN\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "#nltk.download(\"all\")\n",
        "\n",
        "text = \"my name is xyz\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(\"using word tokenize:\",tokens)\n",
        "text2 = \"my name is xyz. I am learning NLP. We are doing first experiment.\"\n",
        "\n",
        "tokens2 = nltk.sent_tokenize(text2)\n",
        "print(\"using sent tokenize:\",tokens2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtYYpjtuSpfC",
        "outputId": "34118171-158f-4f6d-db3d-7fae8e539b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using word tokenize: ['my', 'name', 'is', 'xyz']\n",
            "using sent tokenize: ['my name is xyz.', 'I am learning NLP.', 'We are doing first experiment.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TyJo9j3lX1N-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}